{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5c7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Explain One-Hot Encoding\n",
    "\n",
    "Ans :- One-Hot Encoding is a method of converting the one categorical feature into multiple features with values as 0 and 1.\n",
    "     Let's take an example of gender  which is categorical feature. It will have maximum thre values \n",
    "        which is Male, Female, Others.\n",
    "    After applytin the one hot enconding on gender it converts the it into three columns which is Male, Female and other .\n",
    "    These will have the values of 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Explain Bag of Words\n",
    "Ans: Bag of words is method of converting the sentences into matrix with column heading as unique word and for each\n",
    "    corresponding sentence, if word is present it will be marked as 1 othersiw  0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f5f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Explain Bag of N-Grams\n",
    "Ans : \n",
    "    In bag of words we were considering the only one unique word but in case of N-gram we will consider n \n",
    "    words as unique word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b934c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Explain TF-IDF\n",
    "Ans :\n",
    "    TF - TF is also known as term frequency. It is the ratio of no of repetation of words in a sentence by no of words\n",
    "    in sentence\n",
    "      TF = (No of repetation of a words in a sentence)/(total number of words in sentance)\n",
    "    \n",
    "    IDF :- It is also know as inverse document frequency. It is the log of ratio of no of sentences by\n",
    "        no of sentences containg the word\n",
    "        \n",
    "                   No of sentences\n",
    "        IDF = loge------------------\n",
    "                  no of sentences containg the word\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35122667",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What is OOV problem?\n",
    "Ans: OOV means Out Of Vocabulary. This problem arises when words are not the part of vocabulary after training.\n",
    "    So, oov words are assigned with 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da428ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What are word embeddings?\n",
    "Ans: Word embedding is type of word representaion which allows the word with similar meaning to have \n",
    "    a similar represetation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a72fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Explain Continuous bag of words (CBOW)\n",
    "The continuous bag-of-words (CBOW) model is a neural network for natural languages processing tasks such as language\n",
    "translation and text classification. \n",
    "CBOW predicts a word based on the suronding of the words context in the sentence. \n",
    "The CBOW model takes a window of surrounding words as input and tries to predict the target word in the center of the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b381d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Explain SkipGram\n",
    "\n",
    "Ans : Skipgram is used to predict the context word  for a given target word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb2ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Explain Glove Embeddings.\n",
    "GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for\n",
    "generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. \n",
    "The resulting embeddings show interesting linear substructures of the word in vector space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
